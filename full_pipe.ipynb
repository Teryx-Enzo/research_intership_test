{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitstringNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading bitstring-4.1.4-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting bitarray<3.0.0,>=2.8.0 (from bitstring)\n",
      "  Downloading bitarray-2.9.2-cp311-cp311-win_amd64.whl.metadata (35 kB)\n",
      "Downloading bitstring-4.1.4-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/61.0 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 51.2/61.0 kB 890.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 61.0/61.0 kB 818.3 kB/s eta 0:00:00\n",
      "Downloading bitarray-2.9.2-cp311-cp311-win_amd64.whl (126 kB)\n",
      "   ---------------------------------------- 0.0/126.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 126.0/126.0 kB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: bitarray, bitstring\n",
      "Successfully installed bitarray-2.9.2 bitstring-4.1.4\n"
     ]
    }
   ],
   "source": [
    "pip install bitstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "import bitstring\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cluster import estimate_bandwidth, MeanShift\n",
    "\n",
    "\n",
    "# Get trainticket paths\n",
    "paths = glob('data/eshopper/*.csv')\n",
    "\n",
    "# Shuffle the paths\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(paths)\n",
    "\n",
    "# Create a cycle iterator\n",
    "iterator = cycle(paths)\n",
    "\n",
    "# Initialize the result dictionary\n",
    "results = {'train_set': [], 'test_set': [], 'f1': []}\n",
    "\n",
    "\n",
    "for train_path in iterator:\n",
    "    # Get test path\n",
    "    test_path = next(iterator)\n",
    "\n",
    "    # Get training set name\n",
    "    train_set = train_path.split('/')[-1].split('.')[0]\n",
    "\n",
    "    # Get test set name\n",
    "    test_set = test_path.split('/')[-1].split('.')[0]\n",
    "\n",
    "    # Read train and test data\n",
    "    df_train = pd.read_csv(train_path)\n",
    "    df_test = pd.read_csv(test_path)\n",
    "\n",
    "    # Extract features\n",
    "    X_train = df_train.iloc[:, :-1]\n",
    "    \n",
    "    # Convert labels to binary (0: the request is not affected by performance issues, 1: the request is affected by a performance issue)\n",
    "    y_train = [1 if label in [1, 2] else 0 for label in df_train.iloc[:, -1]]\n",
    "    \n",
    "    # Extract features\n",
    "    X_test = df_test.iloc[:, :-1]\n",
    "    # Convert labels to binary (0: the request is not affected by performance issues, 1: the request is affected by a performance issue)\n",
    "    y_test = [1 if label in [1, 2] else 0 for label in df_test.iloc[:, -1]]\n",
    "\n",
    "    # Train a Decision Tree classifier\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate F1 score (average='weighted' to deal with class imbalance)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Append the result to the dictionary\n",
    "    results['train_set'].append(train_set)\n",
    "    results['test_set'].append(test_set)\n",
    "    results['f1'].append(f1)\n",
    "\n",
    "    # Stop if all the train sets have been used\n",
    "    if len(results['f1']) == len(paths):\n",
    "        break\n",
    "\n",
    "\n",
    "# Create a dataframe from the results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search space contruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [12.612244897959185, 34.89795918367347], 2: [20.0, 43.0], 3: [3.0816326530612246, 4.163265306122449, 5.244897959183674, 6.326530612244898, 24.714285714285715, 43.102040816326536, 44.183673469387756, 45.26530612244898, 46.34693877551021, 47.42857142857143, 48.51020408163266, 52.83673469387755], 4: [75.12244897959184], 5: [10.73469387755102, 22.63265306122449, 38.85714285714286], 6: [16.510204081632654, 33.6530612244898], 7: [181.0, 247.0, 337.0]}\n"
     ]
    }
   ],
   "source": [
    "#df_results\n",
    "X_train[X_train['Latency']>450]\n",
    "\n",
    "\n",
    "#We gonna save the bandwith and the split points determined by the meanshift algo\n",
    "bandwith_dict = {}\n",
    "split_point_dict = {}\n",
    "conversion_RPC_to_index = {}\n",
    "index = 0\n",
    "# For each RPC qui calculate potential split points\n",
    "for column in X_train.columns:\n",
    "\n",
    "    #We don't consider latency as a feature\n",
    "    if column != 'Latency':\n",
    "        index+=1\n",
    "        conversion_RPC_to_index[index] = column\n",
    "        data_rpc_column = X_train[column].values.reshape(-1,1)\n",
    "\n",
    "        bandwith_dict[index] = estimate_bandwidth(data_rpc_column)\n",
    "\n",
    "        clustering = MeanShift(bandwidth=bandwith_dict[index]).fit(data_rpc_column)\n",
    "\n",
    "\n",
    "        # We create a range of points for each rpc to then detect the frontier between the clusters\n",
    "        x_min = np.min(data_rpc_column)\n",
    "        x_max = np.max(data_rpc_column)\n",
    "        range_rpc = np.linspace(x_min,x_max)\n",
    "\n",
    "        n = len(range_rpc)\n",
    "\n",
    "        predict = clustering.predict(range_rpc.reshape(-1,1))\n",
    "\n",
    "        split_points = []\n",
    "\n",
    "        #for each point of the range we only save the frontier values\n",
    "        for i in range(n-1):\n",
    "            if predict[i] != predict[i+1]:\n",
    "                split_points.append(range_rpc[i+1])\n",
    "\n",
    "        split_point_dict[index] = split_points\n",
    "\n",
    "\n",
    "        # TODO  reject clusters without enough points ie -cluster- <|R_pos| * 0.05\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precomputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_R_pos_R_neg(Resquests,Lslo):\n",
    "\n",
    "    \"\"\"\n",
    "    We extract the two subset of request regarding their latency\n",
    "    \n",
    "    \"\"\"\n",
    "    Rpos = X_train[X_train['Latency']>Lslo]\n",
    "    Rneg = X_train[X_train['Latency']<=Lslo]\n",
    "\n",
    "\n",
    "\n",
    "    return Rpos, Rneg\n",
    "\n",
    "    \n",
    "def precomputation(split_point_dict,Rpos,Rneg,conversion_RPC_to_index):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    For each pair <j,t> with j the RPC index and t one of the split point of the RPC we generate a list of \n",
    "    Bool to check if each request is above or under the split point of the considered RPC\n",
    "    \"\"\"\n",
    "    B_pos = {}\n",
    "    B_neg = {}\n",
    "\n",
    "    \n",
    "\n",
    "    for j in split_point_dict.keys():\n",
    "            for t in split_point_dict[j]:\n",
    "\n",
    "\n",
    "                B_pos[(j,t)] = []\n",
    "\n",
    "                for index,request in Rpos.iterrows():\n",
    "                        \n",
    "                    B_pos[(j,t)].append(request[conversion_RPC_to_index[j]]>t)\n",
    "                B_neg[(j,t)] =  []\n",
    "                for index,request in Rneg.iterrows():\n",
    "                    B_neg[(j,t)].append(request[conversion_RPC_to_index[j]]>t)\n",
    "\n",
    "    return B_pos,B_neg\n",
    "\n",
    "\n",
    "def boolList2BinString(liste):\n",
    "    \"\"\" \n",
    "    Just a translator from boolean list to bitstring\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return '0b' + ''.join(['1' if x else '0' for x in liste])\n",
    "\n",
    "def H_pos_H_neg(B_pos,B_neg):\n",
    "\n",
    "    \"\"\" \n",
    "    Final step of the precomputation, we translate each list of Bool to the bit string we'll use later to compute precision and recall\n",
    "    \"\"\"\n",
    "    H_pos, H_neg = {}, {}\n",
    "     \n",
    "    for key, value in B_pos.items():\n",
    "        H_pos[key] = bitstring.BitArray(boolList2BinString(value))\n",
    "    for key, value in B_neg.items():\n",
    "        H_neg[key] = bitstring.BitArray(boolList2BinString(value))\n",
    "     \n",
    "    return H_pos, H_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rpos, Rneg = extract_R_pos_R_neg(X_train,450)\n",
    "\n",
    "H_pos, H_neg = H_pos_H_neg(*precomputation(split_point_dict,Rpos,Rneg,conversion_RPC_to_index))\n",
    "\n",
    "requests = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bts_evaluate_predicate(predicate, H_pos,H_neg):\n",
    "\n",
    "    \"\"\" \n",
    "    For one predicate <j,emin,emax> we get the bitstring corresponding to <j,emin> and <j,emax> in the positive and negative set and apply the\n",
    "    bitwise operation proposed in the article\n",
    "    \"\"\"\n",
    "\n",
    "    j,emin,emax = predicate\n",
    "\n",
    "    B_pos_min, B_pos_max = H_pos[(j,emin)],H_pos[(j,emax)]\n",
    "    B_neg_min, B_neg_max = H_neg[(j,emin)],H_neg[(j,emax)]\n",
    "\n",
    "    B_pos = B_pos_min & (~ B_pos_max)\n",
    "    B_neg = B_neg_min & (~ B_neg_max)\n",
    "    \n",
    "    return B_pos, B_neg\n",
    "\n",
    "def bts_evaluate_pattern(pattern, H_pos,H_neg):\n",
    "    \"\"\"\n",
    "    For each predicate in the pattern we get the evaluation bitstring and then compute the one \n",
    "    corresponding to the pattern\n",
    "    \"\"\"\n",
    "    B_pos_list = []\n",
    "    B_neg_list = []\n",
    "\n",
    "    for predicate in pattern:\n",
    "        B_pos, B_neg = bts_evaluate_predicate(predicate, H_pos,H_neg)\n",
    "        B_pos_list.append(B_pos)\n",
    "        B_neg_list.append(B_neg)\n",
    "\n",
    "    res_B_pos =  B_pos_list[0]\n",
    "    res_B_neg =  B_neg_list[0]\n",
    "\n",
    "    for i in range(1,len(B_pos_list)):\n",
    "        res_B_pos = res_B_pos & B_pos_list[i]\n",
    "        res_B_neg = res_B_neg & B_neg_list[i]\n",
    "\n",
    "    return res_B_pos, res_B_neg\n",
    "\n",
    "def bts_evaluate_pattern_set(pattern_set, H_pos,H_neg):\n",
    "    \"\"\"\n",
    "    For each pattern in the pattern set we get the evaluation bitstring and then compute the one \n",
    "    corresponding to the pattern set\n",
    "    \"\"\"\n",
    "    B_pos_list = []\n",
    "    B_neg_list = []\n",
    "\n",
    "    for pattern in pattern_set:\n",
    "        B_pos, B_neg = bts_evaluate_pattern(pattern, H_pos,H_neg)\n",
    "        B_pos_list.append(B_pos)\n",
    "        B_neg_list.append(B_neg)\n",
    "\n",
    "    res_B_pos =  B_pos_list[0]\n",
    "    res_B_neg =  B_neg_list[0]\n",
    "\n",
    "    for i in range(1,len(B_pos_list)):\n",
    "        res_B_pos = res_B_pos | B_pos_list[i]\n",
    "        res_B_neg = res_B_neg | B_neg_list[i]\n",
    "\n",
    "    return res_B_pos, res_B_neg\n",
    "\n",
    "def compute_tp_fp(pattern_set,H_pos,H_neg):\n",
    "\n",
    "    \"\"\" \n",
    "    We count the number of ones in each bitstring and extrapole precision and recall\n",
    "    \"\"\"\n",
    "    \n",
    "    B_pos, B_neg = bts_evaluate_pattern_set(pattern_set, H_pos,H_neg)\n",
    "    tp,fp,fn = B_pos.int.bit_count() , B_neg.int.bit_count(), B_neg.bin.count('0')\n",
    "\n",
    "\n",
    "    precision = tp / ( tp + fp )\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "def Fbeta_score(pattern_set,beta,H_pos,H_neg):\n",
    "\n",
    "    \"\"\"\n",
    "    From one pattern set, we compute Fbeta socre from precision and recall\n",
    "    \"\"\"\n",
    "\n",
    "    precision, recall = compute_tp_fp(pattern_set,H_pos,H_neg)\n",
    "\n",
    "    score = (1+beta**2)*(precision*recall)/((beta**2 * precision)+recall)\n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "def compute_latency_dissimilarity(pattern_set, H_pos,H_neg,requests):\n",
    "\n",
    "    \"\"\"\n",
    "    for each pattern of the set we stock the latencies of the positive and negative dominated requests and then compute the sum(sum(Lat-mean))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    diss_set = 0\n",
    "    for pattern in pattern_set:\n",
    "        latencies = []\n",
    "        B_pos, B_neg = bts_evaluate_pattern(pattern, H_pos,H_neg)\n",
    "\n",
    "        for i in len(B_pos.bin):\n",
    "            if B_pos[i] : latencies.append(requests.iloc[i]['Latency'])\n",
    "\n",
    "        for i in len(B_neg.bin):\n",
    "            if B_neg[i] : latencies.append(requests.iloc[i]['Latency'])\n",
    "\n",
    "        if latencies :\n",
    "            mean = sum(latencies)/len(latencies)\n",
    "        else : \n",
    "            mean = 0\n",
    "\n",
    "        diss = [(lat-mean)**2 for lat in latencies]\n",
    "        diss_set += diss\n",
    "\n",
    "    return diss_set\n",
    "\n",
    "\n",
    "def fitness(pattern_set, H_pos, H_neg, requests):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    We return the tuple (precision,recall,latency_dissimilarity)\n",
    "    \"\"\"\n",
    "\n",
    "    precision, recall = compute_tp_fp(pattern_set,H_pos,H_neg)\n",
    "    latency_dissimilarity = compute_latency_dissimilarity(pattern_set, H_pos,H_neg,requests)\n",
    "\n",
    "    return precision, recall, latency_dissimilarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "a = bitstring.BitArray('0b0100')\n",
    "\n",
    "print(type(a.bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genectic algorith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class ParetoFront:\n",
    "\n",
    "\n",
    "    def __init__(self,pop_size) -> None:\n",
    "        self.best_pop = np.zeros((pop_size,1))\n",
    "        self.best_fit = np.zeros((pop_size,1))\n",
    "\n",
    "\n",
    "    def update(self,P,P_fitness):\n",
    "\n",
    "        \n",
    "        for individual, fitness in zip(P,P_fitness):\n",
    "\n",
    "\n",
    "            i = np.argmin(self.best_fit)\n",
    "            if fitness > self.best_fit[i]:\n",
    "                self.best_fit[i] = fitness\n",
    "                self.best_pop[i] = individual\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def genetic_algorith(pop_size,gmax,p,q):\n",
    "\n",
    "    PF = None\n",
    "    P = initialise_population(pop_size)\n",
    "    P_fitness = Fbeta(P)\n",
    "    PF.update(P, P_fitness)\n",
    "\n",
    "\n",
    "    for _ in range(gmax):\n",
    "        \n",
    "        Q = []\n",
    "        n = len(P)\n",
    "        for _ in range(n):\n",
    "            r = random.random()\n",
    "\n",
    "            if r < p:\n",
    "                s1,s2 = np.random.choice(P,2,replace = False)\n",
    "                sprime = crossover(s1,s2)\n",
    "            elif r < q+p:\n",
    "                s = np.random.choice(P,1)\n",
    "                sprime = mutation(s,q)\n",
    "            else:\n",
    "                sprime = np.random.choice(P,1)\n",
    "            Q.append(sprime)\n",
    "        \n",
    "        Pprime = P+Q\n",
    "        sorted(P+Q, key =lambda x : rank(x))\n",
    "        P = Pprime[0:n]\n",
    "\n",
    "\n",
    "def rank(population):\n",
    "\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        - population : list of pattern set\n",
    "\n",
    "    Output:\n",
    "        - rank list : rank of each pattern set\n",
    "        - latency dissmilarity : lat_diss of each pattern set \n",
    "    \"\"\"\n",
    "    n = len(population)\n",
    "    fitness = np.empty(n)\n",
    "    eq_dict = {}\n",
    "\n",
    "    rank = 0\n",
    "    rank_list = [0 for _ in range(n)]\n",
    "\n",
    "    pop_copy = population.copy()\n",
    "\n",
    "    for i in range(n):\n",
    "        fitness[i] = fitness(population[i], H_pos, H_neg, requests)\n",
    "        eq_dict[population[i]] = i\n",
    "\n",
    "    while pop_copy:\n",
    "        rank += 1\n",
    "        non_dominated_pattern = []\n",
    "        for pattern_set in pop_copy:\n",
    "            prec, rec, lat = fitness[eq_dict[pattern_set]]\n",
    "            dominated = False\n",
    "            for pattern_dom in pop_copy:\n",
    "                prec_dom, rec_dom, lat_dom = fitness(eq_dict[pattern_dom])\n",
    "                if prec_dom > prec and rec_dom > rec and lat_dom > lat :\n",
    "                    dominated = True\n",
    "                    break\n",
    "            if not dominated:\n",
    "                non_dominated_pattern.append(pattern_set)\n",
    "                rank_list[eq_dict[pattern_set]] = rank\n",
    "        \n",
    "        for element in non_dominated_pattern:\n",
    "            pop_copy.remove(element)\n",
    "\n",
    "\n",
    "    return rank_list, fitness[:,3]\n",
    "    \n",
    "\n",
    "def crossover(s1,s2):\n",
    "\n",
    "    s1copy = s1.copy()\n",
    "    s2copy = s2.copy()\n",
    "    res = []\n",
    "    while s1copy and s2copy:\n",
    "        res.append(s1copy[0],s2copy[0])\n",
    "        s1copy.pop(0)\n",
    "        s2copy.pop(0)\n",
    "    if s1copy:\n",
    "        res += s1copy\n",
    "    elif s2copy:\n",
    "        res += s2copy\n",
    "\n",
    "    cut = int(random.uniform(2,len(res)))\n",
    "    return res[0:cut]\n",
    "\n",
    "\n",
    "def mutation(s,q,pattern_size,split_point_dict):\n",
    "    sprime = s.copy()\n",
    "    r = random.uniform(0,3)\n",
    "\n",
    "    if r<1: #remove one pattern\n",
    "        pattern = random.choice(sprime)\n",
    "        sprime.remove(pattern)\n",
    "    elif r<2: #add one pattern\n",
    "        pattern = generate_pattern(pattern_size,split_point_dict)\n",
    "        sprime.append(pattern)\n",
    "    else: #split one pattern\n",
    "        pattern = random.choice(sprime)\n",
    "        p1,p2 = split(pattern)\n",
    "        sprime.remove(pattern)\n",
    "        sprime.append(p1,p2)\n",
    "\n",
    "    for i in range(len(s)):\n",
    "        r = random.uniform(0,1)\n",
    "        if r<q:\n",
    "            if r<q/2:#remove one predicate\n",
    "                predicate = random.choice(sprime[i])\n",
    "                sprime[i].remove(predicate)\n",
    "\n",
    "            else: #add mutation\n",
    "                predicate = generate_predicate(split_point_dict,len(split_point_dict.keys()))\n",
    "                sprime[i].append(predicate)\n",
    "        \n",
    "\n",
    "\n",
    "    return sprime\n",
    "\n",
    "\n",
    "\n",
    "def split(pattern,split_point_dict):\n",
    "\n",
    "    predicate = generate_predicate(split_point_dict,len(split_point_dict.keys()))\n",
    "    j,e_min,e_max = predicate\n",
    "\n",
    "    Ej = split_point_dict[j]\n",
    "    if predicate in pattern:\n",
    "        e_min_prime = e_min\n",
    "        e_max_prime = e_max\n",
    "        p_prime = pattern.remove(predicate)\n",
    "    else:\n",
    "        e_min_prime = Ej[0]\n",
    "        e_max_prime = Ej[-1]\n",
    "        p_prime = pattern.copy()\n",
    "\n",
    "    Ej = Ej[Ej<e_max_prime]\n",
    "    reduced_Ej = Ej[Ej>=e_min_prime]\n",
    "    t = random.choce(reduced_Ej)\n",
    "\n",
    "    p_1 = (j,e_min_prime,t)\n",
    "    p_2 = (j,t,e_max_prime)\n",
    "    P1 = p_prime + [p_1]\n",
    "    P2 = p_prime + [p_2]\n",
    "\n",
    "\n",
    "    return P1,P2\n",
    "\n",
    "\n",
    "def initialise_population(pop_size,indiv_size,pattern_size,split_point_dict):\n",
    "\n",
    "\n",
    "    pop = np.empty((pop_size,1))\n",
    "\n",
    "    for k in pop_size:\n",
    "        pop[k] = generate_individual(pattern_size,split_point_dict)\n",
    "    \n",
    "\n",
    "    return list(pop)\n",
    "\n",
    "\n",
    "\n",
    "def generate_individual(indiv_size,pattern_size,split_point_dict):\n",
    "    \n",
    "\n",
    "    indiv = np.empty((indiv_size,1))\n",
    "\n",
    "    for k in indiv_size:\n",
    "        indiv[k] = generate_pattern(pattern_size,split_point_dict)\n",
    "    \n",
    "\n",
    "    return list(indiv)\n",
    "\n",
    "def generate_pattern(pattern_size,split_point_dict):\n",
    "    \n",
    "    pattern = np.zeros((pattern_size,1))\n",
    "    nbr_rpc = len(split_point_dict.keys())\n",
    "\n",
    "    for i in range(pattern_size):\n",
    "\n",
    "        pattern[i] = generate_predicate(split_point_dict,nbr_rpc)\n",
    "\n",
    "    \n",
    "    return list(pattern)\n",
    "\n",
    "\n",
    "def generate_predicate(split_point_dict,nbr_rpc):\n",
    "\n",
    "    \n",
    "    j = np.random.uniform(0,nbr_rpc)\n",
    "    index_min = random.choice(range(len(split_point_dict[j])-1))\n",
    "    index_max = random.choice(range(index_min+1,len(split_point_dict[j])))\n",
    "\n",
    "    e_min, e_max = split_point_dict[j][index_min],split_point_dict[j][index_max]\n",
    "\n",
    "    return (j,e_min,e_max)\n",
    "\n",
    "def Fbeta(pop):\n",
    "    \n",
    "\n",
    "    Fbeta_pop = [Fbeta_score(ind,0.1,H_pos,H_neg) for ind in pop]\n",
    "\n",
    "    return  Fbeta_pop\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'remove'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m a\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'remove'"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "\n",
    "a.remove(1)\n",
    "a.append(5)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
