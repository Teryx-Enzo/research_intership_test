{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitstring in c:\\users\\enzo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.1.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: bitarray<3.0.0,>=2.8.0 in c:\\users\\enzo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from bitstring) (2.9.2)\n"
     ]
    }
   ],
   "source": [
    "pip install bitstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "import bitstring\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cluster import estimate_bandwidth, MeanShift\n",
    "\n",
    "\n",
    "# Get trainticket paths\n",
    "paths = glob('data/eshopper/*.csv')\n",
    "\n",
    "# Shuffle the paths\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(paths)\n",
    "\n",
    "# Create a cycle iterator\n",
    "iterator = cycle(paths)\n",
    "\n",
    "# Initialize the result dictionary\n",
    "results = {'train_set': [], 'test_set': [], 'f1': []}\n",
    "\n",
    "\n",
    "for train_path in iterator:\n",
    "    # Get test path\n",
    "    test_path = next(iterator)\n",
    "\n",
    "    # Get training set name\n",
    "    train_set = train_path.split('/')[-1].split('.')[0]\n",
    "\n",
    "    # Get test set name\n",
    "    test_set = test_path.split('/')[-1].split('.')[0]\n",
    "\n",
    "    # Read train and test data\n",
    "    df_train = pd.read_csv(train_path)\n",
    "    df_test = pd.read_csv(test_path)\n",
    "\n",
    "    # Extract features\n",
    "    X_train = df_train.iloc[:, :-1]\n",
    "    \n",
    "    # Convert labels to binary (0: the request is not affected by performance issues, 1: the request is affected by a performance issue)\n",
    "    y_train = [1 if label in [1, 2] else 0 for label in df_train.iloc[:, -1]]\n",
    "    \n",
    "    # Extract features\n",
    "    X_test = df_test.iloc[:, :-1]\n",
    "    # Convert labels to binary (0: the request is not affected by performance issues, 1: the request is affected by a performance issue)\n",
    "    y_test = [1 if label in [1, 2] else 0 for label in df_test.iloc[:, -1]]\n",
    "\n",
    "    # Train a Decision Tree classifier\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate F1 score (average='weighted' to deal with class imbalance)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Append the result to the dictionary\n",
    "    results['train_set'].append(train_set)\n",
    "    results['test_set'].append(test_set)\n",
    "    results['f1'].append(f1)\n",
    "\n",
    "    # Stop if all the train sets have been used\n",
    "    if len(results['f1']) == len(paths):\n",
    "        break\n",
    "\n",
    "\n",
    "# Create a dataframe from the results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search space contruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results\n",
    "X_train[X_train['Latency']>450]\n",
    "\n",
    "\n",
    "#We gonna save the bandwith and the split points determined by the meanshift algo\n",
    "bandwith_dict = {}\n",
    "split_point_dict = {}\n",
    "conversion_RPC_to_index = {}\n",
    "index = 0\n",
    "# For each RPC qui calculate potential split points\n",
    "for column in X_train.columns:\n",
    "\n",
    "    #We don't consider latency as a feature\n",
    "    if column != 'Latency':\n",
    "        index+=1\n",
    "        conversion_RPC_to_index[index] = column\n",
    "        data_rpc_column = X_train[column].values.reshape(-1,1)\n",
    "\n",
    "        bandwith_dict[index] = estimate_bandwidth(data_rpc_column)\n",
    "\n",
    "        clustering = MeanShift(bandwidth=bandwith_dict[index]).fit(data_rpc_column)\n",
    "\n",
    "\n",
    "        # We create a range of points for each rpc to then detect the frontier between the clusters\n",
    "        x_min = np.min(data_rpc_column)\n",
    "        x_max = np.max(data_rpc_column)\n",
    "        range_rpc = np.linspace(x_min,x_max)\n",
    "\n",
    "        n = len(range_rpc)\n",
    "\n",
    "        predict = clustering.predict(range_rpc.reshape(-1,1))\n",
    "\n",
    "        split_points = [x_min]\n",
    "\n",
    "        #for each point of the range we only save the frontier values\n",
    "        for i in range(n-1):\n",
    "            if predict[i] != predict[i+1]:\n",
    "                split_points.append(range_rpc[i+1])\n",
    "\n",
    "        split_points.append(x_max)\n",
    "\n",
    "        split_point_dict[index] = split_points\n",
    "\n",
    "\n",
    "        # TODO  reject clusters without enough points ie -cluster- <|R_pos| * 0.05\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precomputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def extract_R_pos_R_neg(Resquests,Lslo):\n",
    "\n",
    "    \"\"\"\n",
    "    We extract the two subset of request regarding their latency\n",
    "    \n",
    "    \"\"\"\n",
    "    Rpos = X_train[X_train['Latency']>Lslo]\n",
    "    Rneg = X_train[X_train['Latency']<=Lslo]\n",
    "\n",
    "\n",
    "\n",
    "    return Rpos, Rneg\n",
    "\n",
    "    \n",
    "def precomputation(split_point_dict,Rpos,Rneg,conversion_RPC_to_index):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    For each pair <j,t> with j the RPC index and t one of the split point of the RPC we generate a list of \n",
    "    Bool to check if each request is above or under the split point of the considered RPC\n",
    "    \"\"\"\n",
    "    B_pos = {}\n",
    "    B_neg = {}\n",
    "\n",
    "    \n",
    "\n",
    "    for j in split_point_dict.keys():\n",
    "            for t in split_point_dict[j]:\n",
    "\n",
    "\n",
    "                B_pos[(j,t)] = []\n",
    "\n",
    "                for index,request in Rpos.iterrows():\n",
    "                        \n",
    "                    B_pos[(j,t)].append(request[conversion_RPC_to_index[j]]>t)\n",
    "                B_neg[(j,t)] =  []\n",
    "                for index,request in Rneg.iterrows():\n",
    "                    B_neg[(j,t)].append(request[conversion_RPC_to_index[j]]>t)\n",
    "\n",
    "    return B_pos,B_neg\n",
    "\n",
    "\n",
    "def boolList2BinString(liste):\n",
    "    \"\"\" \n",
    "    Just a translator from boolean list to bitstring\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return '0b' + ''.join(['1' if x else '0' for x in liste])\n",
    "\n",
    "def H_pos_H_neg(B_pos,B_neg):\n",
    "\n",
    "    \"\"\" \n",
    "    Final step of the precomputation, we translate each list of Bool to the bit string we'll use later to compute precision and recall\n",
    "    \"\"\"\n",
    "    H_pos, H_neg = {}, {}\n",
    "     \n",
    "    for key, value in B_pos.items():\n",
    "        H_pos[key] = bitstring.BitArray(boolList2BinString(value))\n",
    "    for key, value in B_neg.items():\n",
    "        H_neg[key] = bitstring.BitArray(boolList2BinString(value))\n",
    "     \n",
    "    return H_pos, H_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rpos, Rneg = extract_R_pos_R_neg(X_train,450)\n",
    "\n",
    "H_pos, H_neg = H_pos_H_neg(*precomputation(split_point_dict,Rpos,Rneg,conversion_RPC_to_index))\n",
    "\n",
    "requests = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bts_evaluate_predicate(predicate, H_pos,H_neg):\n",
    "\n",
    "    \"\"\" \n",
    "    For one predicate <j,emin,emax> we get the bitstring corresponding to <j,emin> and <j,emax> in the positive and negative set and apply the\n",
    "    bitwise operation proposed in the article\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    j,emin,emax = predicate\n",
    "\n",
    "    B_pos_min, B_pos_max = H_pos[(j,emin)],H_pos[(j,emax)]\n",
    "    B_neg_min, B_neg_max = H_neg[(j,emin)],H_neg[(j,emax)]\n",
    "\n",
    "    B_pos = B_pos_min & (~ B_pos_max)\n",
    "    B_neg = B_neg_min & (~ B_neg_max)\n",
    "    \n",
    "    return B_pos, B_neg\n",
    "\n",
    "def bts_evaluate_pattern(pattern, H_pos,H_neg):\n",
    "    \"\"\"\n",
    "    For each predicate in the pattern we get the evaluation bitstring and then compute the one \n",
    "    corresponding to the pattern\n",
    "    \"\"\"\n",
    "    B_pos_list = []\n",
    "    B_neg_list = []\n",
    "\n",
    "    for predicate in pattern:\n",
    "        B_pos, B_neg = bts_evaluate_predicate(predicate, H_pos,H_neg)\n",
    "        B_pos_list.append(B_pos)\n",
    "        B_neg_list.append(B_neg)\n",
    "\n",
    "    res_B_pos =  B_pos_list[0]\n",
    "    res_B_neg =  B_neg_list[0]\n",
    "\n",
    "    for i in range(1,len(B_pos_list)):\n",
    "        res_B_pos = res_B_pos & B_pos_list[i]\n",
    "        res_B_neg = res_B_neg & B_neg_list[i]\n",
    "\n",
    "    return res_B_pos, res_B_neg\n",
    "\n",
    "def bts_evaluate_pattern_set(pattern_set, H_pos,H_neg):\n",
    "    \"\"\"\n",
    "    For each pattern in the pattern set we get the evaluation bitstring and then compute the one \n",
    "    corresponding to the pattern set\n",
    "    \"\"\"\n",
    "    B_pos_list = []\n",
    "    B_neg_list = []\n",
    "\n",
    "    for pattern in pattern_set:\n",
    "        B_pos, B_neg = bts_evaluate_pattern(pattern, H_pos,H_neg)\n",
    "        B_pos_list.append(B_pos)\n",
    "        B_neg_list.append(B_neg)\n",
    "\n",
    "    res_B_pos =  B_pos_list[0]\n",
    "    res_B_neg =  B_neg_list[0]\n",
    "\n",
    "    for i in range(1,len(B_pos_list)):\n",
    "        res_B_pos = res_B_pos | B_pos_list[i]\n",
    "        res_B_neg = res_B_neg | B_neg_list[i]\n",
    "\n",
    "    return res_B_pos, res_B_neg\n",
    "\n",
    "def compute_tp_fp(pattern_set,H_pos,H_neg):\n",
    "\n",
    "    \"\"\" \n",
    "    We count the number of ones in each bitstring and extrapole precision and recall\n",
    "    \"\"\"\n",
    "    \n",
    "    B_pos, B_neg = bts_evaluate_pattern_set(pattern_set, H_pos,H_neg)\n",
    "    tp,fp,fn = B_pos.bin.count('1') , B_neg.bin.count('1'), B_neg.bin.count('0')\n",
    "\n",
    "    #print(tp,fp,fn,B_pos, B_neg )\n",
    "    if tp == 0:\n",
    "        precision = recall = 0\n",
    "    else:\n",
    "        precision = tp / ( tp + fp )\n",
    "        recall = tp / (tp + fn)\n",
    "\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "def Fbeta_score(pattern_set,beta,H_pos,H_neg):\n",
    "\n",
    "    \"\"\"\n",
    "    From one pattern set, we compute Fbeta socre from precision and recall\n",
    "    \"\"\"\n",
    "\n",
    "    precision, recall = compute_tp_fp(pattern_set,H_pos,H_neg)\n",
    "    if precision == recall == 0:\n",
    "        score = 0\n",
    "    else :\n",
    "        score = (1+beta**2)*(precision*recall)/((beta**2 * precision)+recall)\n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "def compute_latency_dissimilarity(pattern_set, H_pos,H_neg,requests):\n",
    "\n",
    "    \"\"\"\n",
    "    for each pattern of the set we stock the latencies of the positive and negative dominated requests and then compute the sum(sum(Lat-mean))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    diss_set = 0\n",
    "    for pattern in pattern_set:\n",
    "        latencies = []\n",
    "        B_pos, B_neg = bts_evaluate_pattern(pattern, H_pos,H_neg)\n",
    "\n",
    "        for i in range(len(B_pos.bin)):\n",
    "            if B_pos[i] : latencies.append(requests.iloc[i]['Latency'])\n",
    "\n",
    "        for i in range(len(B_neg.bin)):\n",
    "            if B_neg[i] : latencies.append(requests.iloc[i]['Latency'])\n",
    "\n",
    "        if latencies :\n",
    "            mean = sum(latencies)/len(latencies)\n",
    "        else : \n",
    "            mean = 0\n",
    "\n",
    "        diss = sum([(lat-mean)**2 for lat in latencies])\n",
    "        diss_set += diss\n",
    "\n",
    "    return diss_set\n",
    "\n",
    "\n",
    "def fitness(pattern_set, H_pos, H_neg, requests):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    We return the tuple (precision,recall,latency_dissimilarity)\n",
    "    \"\"\"\n",
    "\n",
    "    precision, recall = compute_tp_fp(pattern_set,H_pos,H_neg)\n",
    "    latency_dissimilarity = compute_latency_dissimilarity(pattern_set, H_pos,H_neg,requests)\n",
    "\n",
    "    return (precision, recall, latency_dissimilarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "a = bitstring.BitArray('0b0100')\n",
    "\n",
    "print(type(a.bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genectic algorith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "\n",
    "class ParetoFront:\n",
    "\n",
    "\n",
    "    def __init__(self,pop_size,split_point_dict) -> None:\n",
    "        self.best_pop = [generate_individual(1,1,split_point_dict) for _ in range(pop_size)]\n",
    "        self.best_fit = [0 for _ in range(pop_size)]\n",
    "\n",
    "\n",
    "    def update(self,P,P_fitness):\n",
    "\n",
    "        \n",
    "        for individual, fitness in zip(P,P_fitness):\n",
    "\n",
    "\n",
    "            i = np.argmin(self.best_fit)\n",
    "            if fitness > self.best_fit[i]:\n",
    "                self.best_fit[i] = fitness\n",
    "                self.best_pop[i] = individual\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def genetic_algorith(pop_size,indiv_size,pattern_size,gmax,p,q):\n",
    "\n",
    "    PF = ParetoFront(pop_size=pop_size, split_point_dict=split_point_dict)\n",
    "    P = initialise_population(pop_size,indiv_size,pattern_size,split_point_dict)\n",
    "    P_fitness = Fbeta(P)\n",
    "    PF.update(P, P_fitness)\n",
    "\n",
    "    # We make gmax generation \n",
    "    for g in range(gmax):\n",
    "        \n",
    "        Q = []\n",
    "        n = len(P)\n",
    "        # For each individual [pattern_set] in the population\n",
    "        for _ in range(n):\n",
    "            r = random.random()\n",
    "            \n",
    "\n",
    "            # crossover\n",
    "            if r < p:\n",
    "            \n",
    "                s1,s2 = sample(P,k=2)\n",
    "                sprime = crossover(s1,s2)\n",
    "\n",
    "            # mutation\n",
    "            elif r < q+p:\n",
    "                s = sample(P,k=1)[0]\n",
    "                sprime = mutation(s,q,pattern_size,split_point_dict)\n",
    "\n",
    "            # selection/reproduction\n",
    "            else:\n",
    "                \n",
    "                sprime = sample(P,k=1)[0]\n",
    "\n",
    "            \n",
    "            Q.append(sprime)\n",
    "        \n",
    "        Pprime = P+Q\n",
    "        rank_list, diss = rank(Pprime)\n",
    "        print(rank_list)\n",
    "        # We sort the total popultion alogn their rank and if ranks are equals the latency dissimilarity is taken into account\n",
    "        Pprime_indexed = [[i,Pprime[i]] for i in range(len(Pprime))]\n",
    "        sorted(Pprime_indexed, key = lambda x : (rank_list[x[0]], diss[x[0]]))\n",
    "\n",
    "        P = [element[1] for element in Pprime_indexed]\n",
    "        \n",
    "        P = P[0:n]\n",
    "\n",
    "\n",
    "        \n",
    "        P_fitness = Fbeta(P)\n",
    "        PF.update(P, P_fitness)\n",
    "        print(\"La taille de la population à la genration {0} : {1}\".format(g,len(P)))\n",
    "\n",
    "        #print(\"La population à la generation {0} : {1}\".format(g,P))\n",
    "        print(\"La fitness à la generation {0} : {1}\".format(g,P_fitness))\n",
    "\n",
    "\n",
    "def rank(population):\n",
    "\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        - population : list of pattern set\n",
    "\n",
    "    Output:\n",
    "        - rank list : rank of each pattern set\n",
    "        - latency dissmilarity : lat_diss of each pattern set \n",
    "    \"\"\"\n",
    "    n = len(population)\n",
    "    fitness_list = np.array([(0.0,0.0,0.0) for _ in range(n)])\n",
    "    eq_dict = {}\n",
    "\n",
    "    rank = 0\n",
    "    rank_list = [0 for _ in range(n)]\n",
    "\n",
    "    pop_copy = population.copy()\n",
    "\n",
    "    for i in range(n):\n",
    "        fitness_list[i] = fitness(population[i], H_pos, H_neg, requests)\n",
    "        eq_dict[str(population[i])] = i\n",
    "\n",
    "    while pop_copy:\n",
    "        rank += 1\n",
    "        non_dominated_pattern = []\n",
    "        for pattern_set in pop_copy:\n",
    "            prec, rec, lat = fitness_list[eq_dict[str(pattern_set)]]\n",
    "            dominated = False\n",
    "            for pattern_dom in pop_copy:\n",
    "                prec_dom, rec_dom, lat_dom = fitness_list[eq_dict[str(pattern_dom)]]\n",
    "                if prec_dom > prec and rec_dom > rec and lat_dom > lat :\n",
    "                    dominated = True\n",
    "                    break\n",
    "            if not dominated:\n",
    "                non_dominated_pattern.append(pattern_set)\n",
    "                rank_list[eq_dict[str(pattern_set)]] = rank\n",
    "        \n",
    "        for element in non_dominated_pattern:\n",
    "            pop_copy.remove(element)\n",
    "\n",
    "\n",
    "    return rank_list, list(fitness_list[:,2])\n",
    "    \n",
    "\n",
    "def crossover(s1,s2):\n",
    "\n",
    "    s1copy = s1.copy()\n",
    "    s2copy = s2.copy()\n",
    "    res = []\n",
    "    while s1copy and s2copy:\n",
    "        res.append(s1copy[0])\n",
    "        res.append(s2copy[0])\n",
    "        s1copy.pop(0)\n",
    "        s2copy.pop(0)\n",
    "    if s1copy:\n",
    "        res += s1copy\n",
    "    elif s2copy:\n",
    "        res += s2copy\n",
    "\n",
    "    cut = int(random.uniform(2,len(res)))\n",
    "    return res[0:cut]\n",
    "\n",
    "\n",
    "def mutation(s,q,pattern_size,split_point_dict):\n",
    "    sprime = s.copy()\n",
    "    r = random.uniform(0,3)\n",
    "\n",
    "    if r<1: #remove one pattern\n",
    "        pattern = random.sample(sprime,k=1)[0]\n",
    "        sprime.remove(pattern)\n",
    "    elif r<2: #add one pattern\n",
    "        pattern = generate_pattern(pattern_size,split_point_dict)\n",
    "        sprime.append(pattern)\n",
    "    else: #split one pattern\n",
    "        pattern = random.sample(sprime,k=1)[0]\n",
    "        p1,p2 = split(pattern,split_point_dict)\n",
    "        sprime.remove(pattern)\n",
    "        sprime.append(p1)\n",
    "        sprime.append(p2)\n",
    "\n",
    "    for i in range(len(sprime)):\n",
    "        r = random.uniform(0,1)\n",
    "        if r<q:\n",
    "            if r<q/2:#remove one predicate\n",
    "                \n",
    "                predicate = random.sample(sprime[i],k=1)[0]\n",
    "                sprime[i].remove(predicate)\n",
    "\n",
    "            else: #add mutation\n",
    "                predicate = generate_predicate(split_point_dict,len(split_point_dict.keys()))\n",
    "                sprime[i].append(predicate)\n",
    "        \n",
    "\n",
    "\n",
    "    return sprime\n",
    "\n",
    "\n",
    "\n",
    "def split(pattern,split_point_dict):\n",
    "\n",
    "    predicate = generate_predicate(split_point_dict,len(split_point_dict.keys()))\n",
    "    j,e_min,e_max = predicate\n",
    "\n",
    "    Ej = split_point_dict[j]\n",
    "    if predicate in pattern:\n",
    "        e_min_prime = e_min\n",
    "        e_max_prime = e_max\n",
    "        p_prime = pattern.copy()\n",
    "        p_prime.remove(predicate)\n",
    "    else:\n",
    "        e_min_prime = Ej[0]\n",
    "        e_max_prime = Ej[-1]\n",
    "        p_prime = pattern.copy()\n",
    "\n",
    "    Ej = [ej for ej in Ej if ej < e_max_prime]\n",
    "    reduced_Ej = [ej for ej in Ej if ej>= e_min_prime]\n",
    "    t = sample(reduced_Ej,k = 1)[0]\n",
    "\n",
    "    p_1 = (j,e_min_prime,t)\n",
    "    p_2 = (j,t,e_max_prime)\n",
    "    P1 = p_prime + [p_1]\n",
    "    P2 = p_prime + [p_2]\n",
    "\n",
    "\n",
    "    return P1,P2\n",
    "\n",
    "\n",
    "def initialise_population(pop_size,indiv_size,pattern_size,split_point_dict):\n",
    "\n",
    "\n",
    "\n",
    "    pop = [generate_individual(indiv_size,pattern_size,split_point_dict) for _ in range(pop_size)]\n",
    "    \n",
    "    \n",
    "    return pop\n",
    "\n",
    "\n",
    "\n",
    "def generate_individual(indiv_size,pattern_size,split_point_dict):\n",
    "    \n",
    "\n",
    "    \n",
    "    indiv = [generate_pattern(pattern_size,split_point_dict) for k in range(indiv_size)]\n",
    "\n",
    "    return indiv\n",
    "\n",
    "def generate_pattern(pattern_size,split_point_dict):\n",
    "    \n",
    "    pattern = [(0.0,0.0,0.0) for _ in range(pattern_size)]\n",
    "    nbr_rpc = len(split_point_dict.keys())\n",
    "\n",
    "    for i in range(pattern_size):\n",
    "        pat = generate_predicate(split_point_dict,nbr_rpc)\n",
    "\n",
    "        pattern[i] = pat\n",
    "\n",
    "    \n",
    "    return list(pattern)\n",
    "\n",
    "\n",
    "def generate_predicate(split_point_dict,nbr_rpc):\n",
    "\n",
    "    \n",
    "    j = int(np.random.uniform(1,nbr_rpc))\n",
    "    \n",
    "    try:\n",
    "        index_min = random.choice(range(len(split_point_dict[j])-1))\n",
    "    except Exception as e:\n",
    "        index_min = 0\n",
    "    try : \n",
    "        index_max = random.choice(range(index_min+1,len(split_point_dict[j])))\n",
    "    except Exception as e:\n",
    "        index_max = 1\n",
    "\n",
    "    try :\n",
    "        e_min, e_max = split_point_dict[j][index_min],split_point_dict[j][index_max]\n",
    "    except Exception as e:\n",
    "        print(e,index_min,index_max,split_point_dict[j])\n",
    "\n",
    "\n",
    "\n",
    "    return (j,e_min,e_max)\n",
    "\n",
    "def Fbeta(pop):\n",
    "    \n",
    "\n",
    "    Fbeta_pop = [Fbeta_score(ind,0.1,H_pos,H_neg) for ind in pop]\n",
    "\n",
    "    return  Fbeta_pop\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [2, 12.612244897959185, 34.89795918367347, 54], 2: [1, 20.0, 43.0, 50], 3: [2, 3.0816326530612246, 4.163265306122449, 5.244897959183674, 6.326530612244898, 24.714285714285715, 43.102040816326536, 44.183673469387756, 45.26530612244898, 46.34693877551021, 47.42857142857143, 48.51020408163266, 52.83673469387755, 55], 4: [41, 75.12244897959184, 117], 5: [1, 10.73469387755102, 22.63265306122449, 38.85714285714286, 54], 6: [1, 16.510204081632654, 33.6530612244898, 41], 7: [103, 181.0, 247.0, 337.0, 397]}\n"
     ]
    }
   ],
   "source": [
    "print(split_point_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "La taille de la population à la genration 0 : 20\n",
      "La fitness à la generation 0 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "La taille de la population à la genration 1 : 20\n",
      "La fitness à la generation 1 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1]\n",
      "La taille de la population à la genration 2 : 20\n",
      "La fitness à la generation 2 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "La taille de la population à la genration 3 : 20\n",
      "La fitness à la generation 3 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "La taille de la population à la genration 4 : 20\n",
      "La fitness à la generation 4 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "genetic_algorith(pop_size = 20, indiv_size=15, pattern_size=15,gmax = 5,p = 0.3,q = 0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
